---
name: Default
version: 1.0.0
schema: v1

models:
  - uses: ollama/qwen2.5-coder-1.5b
    override:
      name: Qwen 2.5 Coder 1.5B
      model: addianto/qwen2.5-coder-abliterated:1.5b

  - name: Qwen 3 Embedding 0.6B
    provider: ollama
    model: hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0
    roles:
      - embed

  - name: Qwen 3 Reranker 0.6B
    provider: ollama
    model: hf.co/mradermacher/Qwen3-Reranker-0.6B-GGUF:Q8_0
    roles:
      - rerank

  - uses: mistral/codestral
    with:
      MISTRAL_API_KEY: ${{ secrets.CODESTRAL_API_KEY }}
    override:
      model: codestral-2508
      defaultCompletionOptions:
        contextLength: 128000

  - uses: mistral/mistral-large
    with:
      MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
    override:
      model: mistral-large-2512
      defaultCompletionOptions:
        contextLength: 256000
      capabilities:
        - tool_use
        - image_input

  - uses: mistral/devstral-medium
    with:
      MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
    override:
      model: devstral-medium-2507
      defaultCompletionOptions:
        contextLength: 128000

  - name: Magistral Medium 1.2
    provider: mistral
    model: magistral-medium-2509
    apiKey: ${{ secrets.MISTRAL_API_KEY }}
    roles:
      - chat
    capabilities:
      - tool_use
      - image_input

  - uses: openai/gpt-4.1
    with:
      # Use API key from GitHub Copilot
      # Refer to https://aider.chat/docs/llms/github.html
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    override:
      name: GPT 4.1 (via GitHub Copilot)
      apiBase: https://api.githubcopilot.com
      model: gpt-4.1

mcpServers:
  - name: Context7
    command: pnpm
    args:
      - "dlx"
      - "@upstash/context7-mcp@1.0.32"

  - name: Sequential Thinking
    command: pnpm
    args:
      - "dlx"
      - "@modelcontextprotocol/server-sequential-thinking@2025.11.25"

  - name: Google Cloud
    command: pnpm
    args:
      - "dlx"
      - "@google-cloud/gcloud-mcp@0.5.2"

  - name: Time
    command: uvx
    args:
      - "mcp-server-time==2025.9.25"

  - name: Git
    command: uvx
    args:
      - "mcp-server-git==2025.11.25"

  # TODO: Figure out why this GitHub MCP server config does not work on Windows.
  #       The Continue plugin on VSCodium unable to call `podman`. Moreover,
  #       calling the binary executable directly from ~/.local/bin also does not work.
  - name: GitHub
    command: podman
    args:
      - "run"
      - "-i"
      - "--rm"
      - "-e"
      - "GITHUB_PERSONAL_ACCESS_TOKEN"
      - "-e"
      - "GITHUB_TOOLSETS"
      - "-e"
      - "GITHUB_LOCKDOWN_MODE"
      - "ghcr.io/github/github-mcp-server:v0.25.0"
    env:
      GITHUB_PERSONAL_ACCESS_TOKEN: ${{ secrets.GITHUB_PERSONAL_ACCESS_TOKEN }}
      GITHUB_TOOLSETS: "default,code_security,secret_protection,security_advisories"
      GITHUB_LOCKDOWN_MODE: "1"
  
  - name: Mermaid
    type: streamable-http
    url: https://mcp.mermaidchart.com/mcp

context:
  - provider: file
  - provider: code
  - provider: diff
  - provider: tree
    params:
      nRetrieve: 25
      nFinal: 5
      useReranking: true
  - provider: docs
    params:
      nRetrieve: 25
      nFinal: 5
      useReranking: true
